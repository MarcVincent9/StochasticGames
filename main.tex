\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\title{Résolution de jeux stochastiques à somme nulle à deux joueurs}
\author{Yoones MIRHOSSEINI, Marc VINCENT\\
				Encadrant : Emmanuel HYON\\
                Master 1 Androïde\\
                Sorbonne Université\\}
                
               

\begin{document}
\maketitle
\tableofcontents 

\section{Descriptif général}
Le sujet de ce projet porte sur les jeux stochastiques à somme nulle à deux joueurs. L’objectif est dans un premier temps de concevoir une modélisation informatique de ces jeux et dans un second temps d’implémenter et comparer des algorithmes permettant de résoudre ces jeux. Les algorithmes testés seront des types suivants : par apprentissage par renforcement et par l’algorithme dit de Shapley avec programmation linéaire. Le but de ces algorithmes est de trouver l’équilibre de Nash en stratégie mixte s’il existe, afin d’en déduire la ou les stratégie(s) optimale(s) pour chaque joueur. Deux jeux serviront à tester nos implémentations : pierre-papier-ciseaux et une version simplifiée du jeu de football.


\section{Définitions}

\subsection{Les processus de décision markoviens}

Dans ce projet, nous faisons usage de la notion de processus de décision markovien (MDP). Un MDP est la formalisation d’un « processus de décision séquentielle dans l’incertain » ~\cite{pdmia}  ; il s’agit formellement d’un « processus stochastique contrôlé satisfaisant la propriété de Markov » ~\cite{pdmia} , défini par~:

\begin{itemize}
\item un espace d’états \(S\) ;
\item un espace d’actions \(A\) ;
\item un axe temporel \(P\) ;
\item une fonction de transition entre états \(T : S \times A \times S \mapsto \Re\) qui donne la probabilité de transition d'un état à un autre en fonction de l'action choisie ;
\item une fonction de récompense \(R : S \times A \mapsto \Re\).
\end{itemize}

Lorsque l’on se trouve dans un état \(s\) et que l’on effectue une action a, on reçoit une récompense \(r(s, a)\) et on a une probabilité \(T(s, a, s')\) de passer à l'état \(s'\). La solution d’un MDP est une stratégie permettant de maximiser les gains obtenus. Le principe d’une stratégie, également appelée «politique» ou «règle de décision», est de fournir l’action que l’on réalisera à chaque étape ; dans le cas d’un MDP, il s’agit toujours d’une stratégie mixte (c’est-à-dire probabiliste) et non pure (déterministe).

Le critère d'évaluation d'une stratégie \(\pi\) est l'espérance de la somme des gains en fonction du sommet de départ \(s\), appelée fonction de valeur. L'une des variantes les plus courantes est de pondérer les gains par un facteur d'actualisation \(\gamma \in [0, 1[\) afin de valoriser le fait d'augmenter ses gains le plus rapidement possible~: 
\[V^\pi(s) = E(\sum\limits_{t=0}^\infty \gamma^t  r_t  | s)\]
où \(r_t\) est la récompense reçue à l'instant \(t\). Cette fonction peut également récursivement de la manière suivante~:
\[V^\pi(s) = r(s, \pi(s)) + \gamma\sum\limits_{s'} T(s, \pi(s), s') V^\pi(s')\]
Le but est de trouver la stratégie optimale \(\pi^* = argmax_\pi V^\pi \). On note \(V^* = max_\pi V^\pi\). L'équation d'optimalité associée, ou équation de point fixe, est la suivante : 
\[\forall s \in S, V^*(s) = max_{a \in A} \{ r(s, a) + \gamma\sum\limits_{s'} T(s, a, s') V^*(s') \} = pf(V^*)\]


\subsection{La théorie des jeux}

Ce projet s’inscrit également dans le large contexte de la théorie des jeux, qui est «un formalisme qui vise à étudier les interactions entre individus »  ~\cite{pdmia} et qui couvre un très large champ d’applications. Dans un jeu, les participants, appelés joueurs, doivent choisir des actions à réaliser afin de maximiser une valeur appelée le gain (fonction d'utilité ou mesure de performance). Nous nous intéressons ici aux jeux non-coopératifs : les joueurs n'ont pas la possibilité de passer des accords entre eux. Lorsque chaque agent n'a qu'une décision à prendre, ce type de jeu peut être formalisé sous la forme suivante :

\begin{itemize}
\item un ensemble de joueurs \(J = \{1, 2, ...\}\)~;
\item pour chaque joueur  \(j \in J\), un ensemble d'actions possibles \(A_i\), avec \(a_i\) l'action choisie par le joueur~;
\item un ensemble des actions conjointes \(A = A_{1} \times A_{2} \times ...\)  où \(A_{j}\) est l'ensemble d'actions du joueur \(j \in J\). On désignera par \(a_j\) l'action du joueur \(j\) et par \(a = (a_1, a_2, ...)\) l'action conjointe des joueurs. De plus, \(a_{-j}\) les actions conjointes de tous les joueurs sauf \(j\)~;
\item une fonction de récompense \(R_{j} : A \mapsto \Re\) pour chaque joueur \(j \in J\) qui donne le gain associé à une action conjointe \(a\).
\end{itemize}

Un tel jeu peut être décrit sous forme stratégique, c'est-à-dire avec une matrice des gains pour les différents joueurs en fonction des actions de chacun.

Nous nous plaçons dans le cadre des jeux à information complète : chaque joueur connaît les actions possibles et les gains de tous les autres joueurs.

La stratégie d'un joueur (ou règle de décision, ou politique) désigne le choix d'action(s) du joueur. Il existe des stratégies pures, c'est-à-dire déterministes (une seule action choisie) et des stratégies mixtes, c'est-à-dire probabilistes : il s'agit d'une distribution de probabilité sur les stratégies pures du joueur \(\pi_j = \Delta A_j\).

Un type de jeu particulièrement étudié est le jeu à somme nulle : dans ce type de jeu la somme des gains de tous les joueurs s'annule : \(\forall a \in A \sum\limits_{j \in J} R_j(a) = 0\). Ce sera le cas des jeux que nous étudierons.

L'étude d'un jeu vise généralement à déterminer les meilleures stratégies possibles des différents joueurs. Différents critères peuvent être utilisés pour cela. En termes de stratégies pures, une stratégie dominante \(a_j \in A_j\) pour le joueur \(j\) est telle que \(a_i\) apporte au joueur un gain plus élevé que toutes ses autres actions possibles quelles que soient les actions de ses adversaires : \(\forall a'_j \neq a_j \forall a_{-j} R_j(a_j, a_{-j}) \geq R_j(a'_j, a_{-j})\). Cette notion permet de définir un équilibre en stratégies dominantes : une action conjointe qui est la combinaison des stratégies dominantes de chaque joueur ; malheureusement la plupart du temps tous les joueurs n'ont pas de stratégie dominante.

L'équilibre de Nash permet de définir de manière plus pratique et plus commune les stratégies optimales d'un jeu. Un équilibre de Nash \(a^{*}\) est une action conjointe dont aucun des joueurs n'a intérêt à dévier : \(\forall j \forall a_j R_j(a^{*}) \geq R_j(a_j, a^{*}_{-j})\). Les équilibres de Nash sont bien plus fréquents que les équilibres en stratégies dominantes, il peut même y en avoir plusieurs dans jeu. La propriété la plus intéressante des équilibres de Nash est que tout jeu en forme stratégique fini admet un équilibre de Nash en stratégies mixtes~\cite{pdmia}. 

Nous avons considéré jusqu'ici des jeux se jouant en un seul tour (une seule action de chaque joueur), mais il existe également des jeux dynamiques, qui se jouent en plusieurs tours. Il y a deux types de jeux dynamiques : les jeux en information parfaite, où les joueurs jouent chacun leur tour, et les jeux répétés, où tous les joueurs  choisissent simultanément leur action. Ces jeux impliquent donc un ensemble de périodes \(P\) : on désigne par \(a^t\) l'action conjointe à une étape \(t \in P\).


\subsection{Les jeux stochastiques}
Les jeux que nous étudierons sont des jeux stochastiques. Les jeux stochastiques sont un modèle réunissant les jeux dynamiques répétés et les MDP. Ce type de jeux implique donc :

\begin{itemize}
\item un ensemble fini d'états \(S\)~;
\item un ensemble fini d'états initiaux \(I \in S\), doté d'une distribution de probabilité \(p_{I}\)~;
\item un ensemble de joueurs \(J = \{1, 2\}\)~;
\item un ensemble d'actions \(A = A_{1} \times A_{2}\)  où \(A_{j}\) est l'ensemble d'actions du joueur \(j \in J\). On désignera par \(a_{j, k}\) la \(k\)-ième action possible du joueur \(j\), par \(a_j\) l'action du joueur \(j\) à une étape donnée et par \(a = (a_1, a_2)\) l'action conjointe des joueurs à une étape donnée~;
\item une fonction de transition \(T : S \times A \times S \mapsto \Re\) qui donne la probabilité de transition d'un état à un autre en fonction de l'action combinée de tous les joueurs~;
\item une fonction de récompense \(R_{j} : S \times A \mapsto \Re\) pour chaque joueur \(j \in J\) qui donne le gain associé à un état et une action combinée~;
\item un ensemble des périodes \(P\)~. A l'instant \(t\), l'état courant sera noté \(s^t\) et l'action choisie  \(a^t\);
\item un facteur d'actualisation \(\gamma \in [0, 1[\).
\end{itemize}

Ainsi, dans un jeu stochastique, chaque action conjointe mène à un nouvel état où se joue l’équivalent d’un jeu statique avec des gains particuliers à chaque état. En théorie, dans un jeu stochastique, les probabilités de transition peuvent dépendre de l’historique de tous les états passés ; lorsque les probabilités de transition ne dépendent que de l’état courant, il s’agit au sens strict d’un jeu de Markov, même si cette distinction n’est pas toujours faite. Dans notre cadre d’étude, on considérera uniquement des jeux où les probabilités de transition ne dépendent que de l’état courant.

Dans ce type de jeux, la stratégie \(\pi_j\) d'un joueur \(j\) sera définie par l'ensemble de ses stratégies \(\pi_j(s)\) pour chaque état \(s\), chacune de ces stratégies "locales" étant une distribution de probabilité sur les actions possibles. On notera  \(\pi_j(s,a)\) la probabilité du joueur \(j\) de jouer \(a\) dans l'état \(s\).

Le but d'un joueur \(j\) sera de maximiser son espérance temporelle, définie par~:
\[U_j^\pi(s) = E(\sum\limits_{t=0}^\infty \gamma^t u_j^{\pi_j}(s)^t | s^0=s) = u_j^{\pi_j}(s) + \gamma \sum\limits_{a \in A}\sum\limits_{s' \in S}T(s,a,s')\pi_j(s,a)U_j^\pi(s')\] avec \(u_j^{\pi_j}(s) = E(R_j(s, a)|a \in A, \pi)\) l'utilité espérée.

En notant \(\Pi_j\) l'ensemble des stratégies possibles du joueur \(j\), on définit dans un jeu stochastique un équilibre de Nash \(\pi^*\) par :  \[\forall s \in S,   \forall j \in J,   \forall \pi_j \in \Pi_j,   U_j^{\pi^*}(s) \geq U_j^{\pi_j, \pi^*_{-j}}(s)\] avec \(\Pi_j\) l'ensemble des stratégies possibles du joueur \(j\).

Étant donné que dans notre cadre d’étude, on considérera uniquement des jeux où les probabilités de transition ne dépendent que de l’état courant, nous chercherons des stratégies stationnaires, c'est-à-dire que le choix des actions dépendra uniquement de l'état courant. Tout jeu stochastique possède au moins un équilibre de Nash en stratégie stationnaire ~\cite{pdmia}.

Nous testerons nos algorithmes sur des jeux stochastiques à somme nulle à deux joueurs, mais notre code sera générique aux jeux stochastiques en général.




\subsection{Jeux-jouets}
Les jeux-jouets sur lesquels nous testerons nos approches sont des exemples classiques des jeux stochastiques : 
\begin{itemize}
\item pierre-papier-ciseaux est un exemple de jeu stochastique à un état, avec 3 actions disponibles, soit pratiquement le modèle le plus simple possible. C’est ainsi un cas particulier où il n’y a pas de part d’aléatoire dans l’évolution du jeu, c’est donc l’équivalent d’un jeu dynamique répété ;
\item le jeu de football simplifié implique deux joueurs et un ballon sur un terrain composé de 4x5 cases. Chaque agent contrôle un joueur et a au maximum 5 actions possibles (déplacement vers une case adjacente ou immobilité) : les 2 actions sont choisies simultanément mais l’ordre dans lequel elle s’exécute est aléatoire. Le ballon se déplace en fonction des actions des joueurs : lorsqu'un joueur essaye d'aller dans la case où se trouve l'autre joueur, le ballon revient systématiquement au joueur immobile. Comme l'ordre des déplacements est aléatoire, la détermination de la possession du ballon se fait de manière aléatoire dans certaines situations. Ce jeu a ceci d'intéressant qu'il y a deux états initiaux : les cases de départ des joueurs sont les mêmes dans les deux cas mais la possession du ballon est décidée aléatoirement. Le but est évidemment d’aller porter le ballon à une extrémité du terrain, ce qui rapporte une récompense de 1 au buteur et ramène à l'un des deux états initiaux ~\cite{littman} ;
\item l’attaque-défense dans un réseau électrique : un réseau électrique peut être représenté par un graphe avec des nœuds qui créent de l’électricité, des nœuds qui en consomment et des arcs qui assurent le transport de l’électricité. Le but d’un attaquant est de minimiser la quantité d’électricité transportée, celui d’un défenseur est de la maximiser. L’action à chaque tour d’un attaquant est d’endommager un nombre limité de liens entre les nœuds du réseau, celle d’un défenseur est de le contrer en réparant ou en renforçant un nombre limité de liens. Pour chaque paire d’actions, la transition vers le nouvel état s’effectue selon une distribution de probabilité. ~\cite{att-def}
\end{itemize}


\section{Algorithmes}

Plusieurs algorithmes ont été proposés pour résoudre les jeux stochastiques à somme nulle. Leur principe général est de calculer la valeur optimale du jeu pour déterminer le ou les équilibre(s) de Nash et donc les stratégies optimales de chaque joueur. Pour certains de ces algorithmes, il n’existe pas de preuve qu’ils convergent; pour d’autres, il a été prouvé qu’il existait des conditions, très restrictives, sous lesquels ils convergeaient nécessairement, mais pas forcément vers un équilibre de Nash. Nous accorderons donc une attention particulière à la convergence des algorithmes que nous testerons et à la vérification de leur solution. En revanche, à l’heure actuelle, il existe très peu d’algorithmes pour les jeux stochastiques à somme non nulle. Nous détaillons ci-dessous les types d’algorithmes que nous étudierons.

\subsection{Algorithme de Shapley avec programmation linéaire}
Dans un premier temps nous allons utiliser et implémenter l'algorithme de Shapley [SHA 4] pour résoudre les jeux stochastiques à deux joueurs à somme nulle. Cet algorithme trouve un équilibre de Nash, puis à partir de cet équilibre de Nash on est capable de calculer les stratégies optimales pour chaque joueur.

Cet algorithme fonctionne selon le principe de l’itération sur les valeurs : à chaque itération, pour chaque état de jeu possible, il faut calculer la valeur du jeu, et ce jusqu’à convergence pour tous les états, pour finalement calculer des stratégies optimales à partir des valeurs trouvées. L’algorithme originel utilise l’énumération de Snow et Shapley qui est très coûteuse en temps. Pour résoudre ce problème nous allons utiliser la programmation linéaire pour calculer la valeur du jeu et ainsi que les stratégies optimales. 

La programmation linéaire est une technique mathématique d'optimisation de fonction objectif linéaire sous des contraintes ayant la forme d'inéquations (ou équations) linéaires. [BAU 5] Pour calculer la valeur de jeu à chaque étape (état du jeu) nous allons résoudre par programmation linéaire la fonction ci-dessous après transformation en forme linéaire :
		 
oùest la stratégie du joueur 1 (respectivement 2) etl’ensemble des stratégies du joueur 1 (resp. 2),l’action effectuée par le joueur 1 (resp. 2),l’ensemble des actions disponibles pour le joueur 1,la récompense obtenue et finalementla probabilité de choisir l’actiondans la stratégie. v(s) est la valeur du jeu pour l’état s.

\subsection{Apprentissage par renforcement}
Les méthodes d’apprentissage par renforcement permettent de traiter les situations dans lesquelles les fonctions de transition et de récompense d’un MDP ne sont pas connues a priori ~\cite{pdmia}. La plupart de ces méthodes s’inspirent des principes de l’apprentissage humain ou animal : renforcer la tendance à exécuter une action dont les conséquences sont jugées positives, prendre en compte la durée qui sépare la récompense de l’action ou la fréquence à laquelle cette action a été testée. ~\cite{pdmia} L’apprentissage par renforcement est donc une forme d’apprentissage non supervisé reposant sur une évaluation, qui implique un fonctionnement par essais et erreurs. Ainsi, « l’apprentissage par renforcement est basé sur une interaction itérée du système apprenant avec l’environnement, à partir de laquelle une politique est progressivement améliorée. Cependant, en pratique, la plupart des algorithmes d’apprentissage par renforcement ne travaillent pas directement sur la politique, mais passent par l’approximation itérative d’une fonction de valeur ». ~\cite{pdmia}

L’algorithme d’apprentissage par renforcement que nous allons implémenter, proposé par Littman ~\cite{littman} et connu sous le nom de «Minimax Q-learning », est une extension de l’algorithme de Q-learning standard. Le Q-learning est une méthode d’apprentissage par renforcement qui consiste à apprendre la valeur des actions selon les états, ce qui permet de calculer les utilités et la politique optimale dynamiquement.

En Q-learning, on associe à chaque couple état-action d’un agent une Q-valeur Q(s, a) qui correspond à « la récompense espérée obtenue en effectuant l’action a dans l’état s et en suivant une politique optimale à partir de l’état suivant. » ~\cite{gie-cha} Ces valeurs sont évaluées dynamiquement par l’expérience de l’agent dans l’environnement. Le Q-learning est un algorithme d’apprentissage mono-agent, qui peut être adapté pour un environnement multi-agent mais sans prendre en compte explicitement la présence des autres agents : en effet, dans ce cas, l’autre agent est considéré comme une partie de l’environnement.

Le principal problème que pose l’application directe du Q-learning est le suivant : dans le cas où l’adversaire a une stratégie complète et complexe, il est possible que l’algorithme ne converge pas, ce qui nous empêche de trouver la stratégie optimale. L’algorithme Minimax Q-learning essaye de résoudre ce problème et de prendre en compte de manière plus explicite la présence de l’autre agent en utilisant la technique du minimax de la théorie des jeux.



\section{Modélisation et conception}

\subsection{Conception}
Dans cette partie, nous exposons et justifions nos choix en matière d'implémentation. Le but du logiciel que nous codons est double : il doit répondre à nos besoins, que nous détaillons ci-après, et doit pouvoir être réutilisé et amélioré par d'autres personnes travaillant sur les jeux stochastiques.

L'objectif principal de ce projet est de tester et comparer des algorithmes de résolution de jeux stochastiques à somme nulle à deux joueurs. Pour cela, nous avons besoin d'implémenter un logiciel permettant d'appliquer ces algorithmes à plusieurs jeux. Pour effectuer des tests sur ces jeux, il faut donner une représentation informatique des jeux stochastiques et coder nos algorithmes de manière à ce qu'ils s'appliquent à celle-ci. Bien que notre projet se limite aux jeux à somme nulle à deux joueurs, nous nous efforçons de donner une représentation générique des jeux stochastiques, en vue d'extensions ultérieures, ce qui concerne principalement le nombre de joueurs. Il s'agit donc en quelque sorte de définir un langage de description ou \textit{DSL} pour les jeux stochastiques en général. Nous mettons en particulier l'accent sur l'utilisabilité de ce logiciel, la façon dont l'utilisateur devra entrer un jeu et lancer un algorithme dessus.

Au cours de la conception de notre implémentation, il a fallu que nous apportions des solutions à plusieurs problèmes, quitte à dévier par moments de la modélisation mathématique. Les principaux problèmes concernaient :
\begin{itemize}
\item la représentation externe, c'est-à-dire la manière dont l'utilisateur entrera les données d'un jeu~;
\item la représentation interne, à savoir les structures de données nécessaires pour contenir toutes les informations qui définissent un jeu~;
\item la modélisation des états, des actions et des joueurs, et la manière de les "identifier"~;
\item la gestion des actions impossibles dans certains états~;
\item le fait que dans la plupart des jeux envisageables, pour la grande majorité des paires d'états \((s, s')\), il n'y a pas de transition possible, c'est-à-dire : \(\forall a\) \(T(s, a, s') = 0\).
\end{itemize}

Les deux derniers points peuvent être illustrés par l'exemple du jeu de football simplifié, qui nous a en grande partie guidés lors de cette phase de conception. Ils peuvent.

Tout d'abord, il a fallu déterminer les grandes lignes de ces représentations. Au niveau de la représentation interne, nous avons tout d'abord pensé intuitivement à des représentations matricielles. Ainsi, la première approche qui a été proposée était de stocker pour chaque paire d'états \((s, s')\) une matrice carrée \(M\) avec les actions de chaque joueur en entrée telle que \(M[a_1, a_2] = T(s, (a_1, a_2), s')\) ; et pour chaque état, stocker une matrice similaire pour les récompenses immédiates. Cette approche a très vite été écartée en raison du fait que ces matrices auraient été très largement vides, utilisant ainsi de l'espace mémoire inutile. Une seconde approche se proposait de stocker pour chaque état une matrice des actions contenant pour chaque action combinée la récompense immédiate associée et une distribution de probabilité sur l'état suivant, celle-ci étant limitée aux états accessibles. Cette solution, nettement plus optimisée, semblait néanmoins trop complexe, trop peu générique, peu propice aux modifications ultérieures ; elle posait également la question de la modélisation des états et de leur identification.

En effet, la modélisation des actions et des joueurs peut se limiter à un simple identifiant (chaîne de caractères ou entier) alors que les états sont en général assez nombreux (760 dans le jeu de football) : si l'on modélisait les états par un identifiant, il ne serait pas trivial de déterminer les identifiants des états accessibles depuis un état donné. Par conséquent, nous avons estimé que les états complexes devraient pouvoir être modélisés sous forme de tuples pour faciliter la détermination des états accessibles : pour le jeu de football, un état serait (position du joueur 1, position du joueur 2, joueur en possession du ballon). Il nous est alors apparu que dans les exemples du jeu de football et de l'attaque-défense sur un réseau électrique, et par extension dans les autres jeux envisageables, il était nécessaire que les états accessibles soient générés automatiquement plutôt que d'être rentrés à la main à l'utilisateur : de même pour les distributions de probabilité sur ceux-ci et pour les récompenses immédiates. Cela nous amène au point suivant sur la représentation externe.

En tentant de formaliser dans un premier temps le jeu de football d'après la modélisation mathématique, nous avons constaté que la manière la plus simple pour l'utilisateur d'entrer les transitions et les récompenses dans le logiciel serait d'écrire les fonctions associées ; par ailleurs, concernant la représentation interne, il se trouve que c'est également la solution la moins coûteuse en mémoire et qu'elle fournit en plus l'API nécessaire aux algorithmes. Cette approche peut de plus être utilisée pour toutes les autres données d'un jeu : donner la liste des états, celle des actions, celle des joueurs, le facteur d'actualisation et la distribution de probabilité sur les états initiaux. L'implémentation la plus générique est donc de définir une classe abstraite \textit{StochasticGames} contenant toutes ces méthodes, que l'utilisateur devrait implémenter pour obtenir une classe définissant un jeu particulier. Le fait de modéliser un jeu par une classe permet en outre de définir des attributs propres au jeu et utilisables dans différentes méthodes (notamment le graphe dans l'exemple de l'attaque-défense dans un réseau électrique). L'inconvénient ici du langage choisi pour l'implémentation, Python, est qu'il ne permet pas (en tout cas de manière directe) de vérifier que l'utilisateur implémente les méthodes avec la bonne signature : le mieux que l'on puisse faire est de fournir la documentation la plus claire possible.

Cependant, la signature de la fonction de transition \(T : S \times A \times S' \mapsto \Re\) pose problème. Le problème de l'absence de transition pour la plupart des paires d'états est évité par cette solution ; en revanche, l'exemple du jeu de football nous montre qu'elle peut ne pas être naturelle à coder ; mais surtout, dans les algorithmes basés sur la simulation (notamment Q-learning) et pour les tests, étant donné un état et une action combinée, la détermination de l'état suivant nécessiterait de parcourir l'ensemble des états pour trouver la probabilité de transition pour chacun d'entre eux. Afin de simplifier l'écriture de la fonction et d'optimiser le calcul d'une transition, nous proposons que la fonction de transition prenne uniquement en entrée l'état courant et l'action et qu'elle renvoie une distribution de probabilité sur l'état suivant, sous la forme d'un dictionnaire.

La question de la gestion des actions impossibles dans certains états s'est également posée. Dans un premier temps nous avions pensé effectuer ces vérifications dans les fonctions de transition et de récompense, notamment en attribuant une récompense équivalente à \(-\infty\) ; cependant, cette solution aurait pu poser problème sur certains algorithmes. La solution retenue, plus naturelle pour les utilisateurs, consiste à définir les actions possibles selon l'état dans la fonction renvoyant les actions : celle-ci prend ainsi en paramètres le joueur et l'état courant et renvoie les actions possibles pour ce joueur dans cet état.

Par ailleurs, nous avions dans un premier temps mis dans l'interface une méthode donnant le nombre de joueurs \(n\) : on attribuait automatiquement aux joueurs un identifiant dans \([1, n-1]\). Cependant, pour permettre une implémentation plus libre et par cohérence avec les fonctions pour les actions et les états, nous l'avons remplacée par une fonction devant renvoyer la liste des joueurs (entiers ou chaînes de caractères). Par conséquent, la donnée des actions ou des récompenses de l'ensemble des joueurs à un temps donné est représentée par un dictionnaire associant une valeur à chaque joueur, là où auparavant on se contentait d'un tuple.

Dans notre cadre d'étude, certaines simplifications peuvent être faites par rapport à l'implémentation générique décrite jusqu'ici : il n'y a que deux joueurs (0 et 1) et la seule fonction de récompense dont nous ayons besoin est celle qui donne la récompense du joueur 0, la récompense de l'autre joueur pouvant être déduite puisque nous étudions des jeux à somme nulle. Nous mettons en place ces simplifications en créant une sous classe \textit{NullSum2PlayerStochasticGame} de notre classe \textit{StochasticGame}. Néanmoins un problème se pose dans le cadre générique pour la fonction de récompense : la cohérence avec le reste de l'API voudrait que cette fonction prenne un joueur en paramètre et renvoie la récompense de celui-ci, mais pour permettre l'optimisation dans les jeux à somme nulle il faut introduire une fonction rendant les récompenses de l'ensemble des joueurs..............................................................................

Enfin, concernant l'implémentation des algorithmes, nous avons décidé de stocker \(\pi\) et \(V\) sous forme de dictionnaires ; l'avantage par rapport à une matrice est que le dictionnaire permet d'utiliser n'importe quels identifiants pour les joueurs, les actions et les états (entier, chaîne de caractères ou tuple) et qu'il évite les "creux" qui auraient été présents dans une matrice.



\bibliography{stocha}{}
\bibliographystyle{plain}
\end{document}
